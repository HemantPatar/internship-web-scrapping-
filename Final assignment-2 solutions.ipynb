{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEB SCRAPING ASSIGNMENT 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.webdriver.common.action_chains import ActionChains \n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1. Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. Scrape the job-title, job-location, company_name, experience_required. Scrape first 10 jobs data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Definition\n",
    "def data_sci(url):\n",
    "    job_desp, company, location, experience = [], [], [], []\n",
    "    driver=webdriver.Chrome(r'D:\\chromedriver.exe')\n",
    "    driver.get(url)\n",
    "    search_job = driver.find_element_by_xpath(\"//input[@id='qsb-keyword-sugg']\")\n",
    "    search_job.send_keys('Data Analyst')\n",
    "    search_loc = driver.find_element_by_xpath(\"//input[@id='qsb-location-sugg']\")\n",
    "    search_loc.send_keys('Bangalore')\n",
    "    search_button=driver.find_element_by_xpath(\"//div[@class='search-btn']/button\")\n",
    "    search_button.click()\n",
    "    time.sleep(5)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    jobs = soup.find_all('div', attrs ={'class':'info fleft'})\n",
    "    for x, i in enumerate(jobs):\n",
    "        for y, j in enumerate(i.find_all('a')):\n",
    "            if \"Reviews\" in str(j.text): pass\n",
    "            else:\n",
    "                if y+1 == 1: \n",
    "                    job_desp.append(j.text)\n",
    "                else: company.append(j.text)\n",
    "        for z, k in enumerate(i.find('ul', attrs = {\"class\": \"mt-7\"}).find_all(\"li\")):\n",
    "            if z+1 == 2: pass\n",
    "            else: \n",
    "                if z+1 == 1: experience.append(k.text)\n",
    "                else: location.append(k.text)\n",
    "    driver.quit()\n",
    "    df=pd.DataFrame({'Job Title':job_desp[:10],\n",
    "                     'Company':company[:10],\n",
    "                     'Experiance':experience[:10],\n",
    "                    'Location':location[:10]})\n",
    "    print(df)\n",
    "    df.to_csv('naukri_data_analyst_10.csv', index = False)\n",
    "\n",
    "#     print(job_desp)\n",
    "#     print(company)\n",
    "#     print(experience)\n",
    "#     print(location)\n",
    "\n",
    "# Calling Function\n",
    "data_sci(\"https://www.naukri.com/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, full job-description. You have to scrape first 10 jobs data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           Job Title  \\\n",
      "0  Fresher  Data Engineer / Data Scientist / Data...   \n",
      "1  Immediate opening For Data Scientist/Data Analyst   \n",
      "2                      Data Scientist / Data Analyst   \n",
      "3             Senior Data Scientist - NLP/ Python/ R   \n",
      "4           Explore job openings on Data Scientist!!   \n",
      "5  Lead Data Scientist - Machine Learning/ Data M...   \n",
      "6                              Senior Data Scientist   \n",
      "7                  Data Scientist - Machine Learning   \n",
      "8     Artificial Intelligence Analyst/Data Scientist   \n",
      "9                                     Data Scientist   \n",
      "\n",
      "                                             Company  \\\n",
      "0                     ACHYUTAS SOFT PRIVATE LIMITED    \n",
      "1  CAIA-Center For Artificial Intelligence & Adva...   \n",
      "2                          Altimetrik India Pvt. Ltd   \n",
      "3                                 AVI Consulting LLP   \n",
      "4                          Bristlecone India Limited   \n",
      "5                       Wrackle Technologies Pvt Ltd   \n",
      "6                                            Signify   \n",
      "7                  BLUE YONDER INDIA PRIVATE LIMITED   \n",
      "8                            TalentCo Search Pvt Ltd   \n",
      "9                            Mailkit Private Limited   \n",
      "\n",
      "                                         Description  \\\n",
      "0  Roles and ResponsibilitiesAnalytical Skills:To...   \n",
      "1   Dear Candidate  Schedule a Telephonic Intervi...   \n",
      "2      3+ years of data science,         data ana...   \n",
      "3  Roles and ResponsibilitiesSkill : NLP,Semantic...   \n",
      "4   JOB DESCRIPTION  Exp  4+ years 1) Hands-on py...   \n",
      "5  Roles and ResponsibilitiesRequirements :- 6-9 ...   \n",
      "6   We re on the lookout for forward-thinking inn...   \n",
      "7  The Yantriks Data Science and Machine Learning...   \n",
      "8  Roles and Responsibilities- Machine Learning t...   \n",
      "9  Mailkit is an European Marketing Automation co...   \n",
      "\n",
      "                                            Location  \n",
      "0                    Delhi NCR, Bengaluru, Hyderabad  \n",
      "1                Chennai, Pune, Bengaluru, Hyderabad  \n",
      "2                                          Bengaluru  \n",
      "3                               Bengaluru, Hyderabad  \n",
      "4                            Pune, Mumbai, Bengaluru  \n",
      "5                                          Bengaluru  \n",
      "6                                          Bengaluru  \n",
      "7                                          Bengaluru  \n",
      "8                                  Mumbai, Bengaluru  \n",
      "9  Chennai, Pune, Mumbai, Bengaluru, Hyderabad, K...  \n"
     ]
    }
   ],
   "source": [
    "# Function Definition\n",
    "def data_sci(url):\n",
    "    job_desp, company, location, description, urls = [], [], [], [],[]\n",
    "    driver=webdriver.Chrome(r'D:\\chromedriver.exe')\n",
    "    driver.get(url)\n",
    "    search_job = driver.find_element_by_xpath(\"//input[@id='qsb-keyword-sugg']\")\n",
    "    search_job.send_keys('Data Scientist')\n",
    "    search_loc = driver.find_element_by_xpath(\"//input[@id='qsb-location-sugg']\")\n",
    "    search_loc.send_keys('Bangalore')\n",
    "    search_button=driver.find_element_by_xpath(\"//div[@class='search-btn']/button\")\n",
    "    search_button.click()\n",
    "    time.sleep(5)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    jobs = soup.find_all('div', attrs ={'class':'info fleft'})\n",
    "\n",
    "    for x, i in enumerate(jobs):\n",
    "        for y, j in enumerate(i.find_all('a')):\n",
    "            if \"Reviews\" in str(j.text): pass\n",
    "            else:\n",
    "                if y+1 == 1: \n",
    "                    job_desp.append(j.text)\n",
    "                    urls.append(j.get('href'))\n",
    "                else: company.append(j.text)\n",
    "        for z, k in enumerate(i.find('ul', attrs = {\"class\": \"mt-7\"}).find_all(\"li\")):\n",
    "            if z+1 == 3: location.append(k.text)\n",
    "            else: pass\n",
    "    \n",
    "    for url in urls:\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        soup1 = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        jobs_d = soup1.find('div', attrs ={'class':['dang-inner-html','clearboth description']})\n",
    "        description.append(jobs_d.text)\n",
    "    \n",
    "    driver.close()\n",
    "    driver.quit()\n",
    "    df=pd.DataFrame({'Job Title':job_desp[:10],\n",
    "                     'Company':company[:10],\n",
    "                     'Description':description[:10],\n",
    "                    'Location':location[:10]})\n",
    "    print(df)\n",
    "    df.to_csv('naukri_data_scientist_10.csv', index = False)\n",
    "#     print(job_desp)\n",
    "#     print(company)\n",
    "#     print(description)\n",
    "#     print(location)\n",
    "#     print(urls)\n",
    "\n",
    "# Calling Function\n",
    "data_sci(\"https://www.naukri.com/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3: webpage https://www.naukri.com/ scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           Job Title  \\\n",
      "0  Fresher  Data Engineer / Data Scientist / Data...   \n",
      "1           Data Scientist - Python/Machine Learning   \n",
      "2                                     Data Scientist   \n",
      "3  Only Fresher / Data Scientist / Data Analyst /...   \n",
      "4                                     Data Scientist   \n",
      "5                                     Data Scientist   \n",
      "6                                Lead Data Scientist   \n",
      "7  Excellent opportunity For Lead Data Scientist ...   \n",
      "8                                Lead Data Scientist   \n",
      "9  Excellent opportunity For Lead Data Scientist ...   \n",
      "\n",
      "                             Company Experiance  \\\n",
      "0     ACHYUTAS SOFT PRIVATE LIMITED     0-2 Yrs   \n",
      "1                              Jubna    5-8 Yrs   \n",
      "2             IBM India Pvt. Limited    4-8 Yrs   \n",
      "3         GABA Consultancy services     0-0 Yrs   \n",
      "4              PureSoftware Pvt Ltd.    5-9 Yrs   \n",
      "5             Air Asia India Limited    1-4 Yrs   \n",
      "6  NEC CORPORATION INDIA PRIVATE LTD   9-14 Yrs   \n",
      "7  NEC CORPORATION INDIA PRIVATE LTD   9-14 Yrs   \n",
      "8  NEC CORPORATION INDIA PRIVATE LTD   9-14 Yrs   \n",
      "9  NEC CORPORATION INDIA PRIVATE LTD   9-14 Yrs   \n",
      "\n",
      "                             Location  \n",
      "0     Delhi NCR, Bengaluru, Hyderabad  \n",
      "1                               Noida  \n",
      "2                    Gurgaon Gurugram  \n",
      "3     Delhi NCR, Greater Noida, Noida  \n",
      "4                             Gurgaon  \n",
      "5                  Delhi NCR, Gurgaon  \n",
      "6  Delhi NCR, Noida(Sector-142 Noida)  \n",
      "7  Delhi NCR(Sector-142 Noida), Noida  \n",
      "8  Delhi NCR, Noida(Sector-142 Noida)  \n",
      "9  Delhi NCR(Sector-142 Noida), Noida  \n"
     ]
    }
   ],
   "source": [
    "driver=webdriver.Chrome(executable_path='D:\\chromedriver.exe')\n",
    "# Function Definition\n",
    "def data_sci(url):\n",
    "    job_desp, company, location, experience = [], [], [], []\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "\n",
    "    driver.find_element_by_xpath(\"//input[@id='qsb-keyword-sugg']\").send_keys('Data Scientist')\n",
    "    time.sleep(3)\n",
    "\n",
    "    driver.find_element_by_xpath(\"//div[@class='search-btn']/button\").click()\n",
    "    time.sleep(2)\n",
    "    \n",
    "    j_loc = driver.find_element_by_xpath('/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[2]/div[2]/div[2]/label/i')\n",
    "    action = ActionChains(driver) \n",
    "    action.click(on_element=j_loc)\n",
    "    action.perform() \n",
    "    time.sleep(3)\n",
    "    \n",
    "    j_sal = driver.find_element_by_xpath('/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[3]/div[2]/div[2]/label/i')\n",
    "    action = ActionChains(driver) \n",
    "    action.click(on_element=j_sal) \n",
    "    action.perform() \n",
    "    time.sleep(3)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    jobs = soup.find_all('div', attrs ={'class':'info fleft'})\n",
    "    for x, i in enumerate(jobs):\n",
    "        for y, j in enumerate(i.find_all('a')):\n",
    "            if \"Reviews\" in str(j.text): pass\n",
    "            else:\n",
    "                if y+1 == 1: \n",
    "                    job_desp.append(j.text)\n",
    "                else: company.append(j.text)\n",
    "        for z, k in enumerate(i.find('ul', attrs = {\"class\": \"mt-7\"}).find_all(\"li\")):\n",
    "            if z+1 == 2: pass\n",
    "            else: \n",
    "                if z+1 == 1: experience.append(k.text)\n",
    "                else: location.append(k.text)\n",
    "    \n",
    "    df=pd.DataFrame({'Job Title':job_desp[:10],\n",
    "                     'Company':company[:10],\n",
    "                     'Experiance':experience[:10],\n",
    "                    'Location':location[:10]})\n",
    "    print(df)\n",
    "    df.to_csv('naukri_data_scientist_filtered_10.csv', index = False)\n",
    "\n",
    "# Calling Function\n",
    "try:\n",
    "    data_sci(\"https://www.naukri.com/\")\n",
    "    driver.quit()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4: Write a python program to scrape data for first 10 job results for Data scientist Designation in Noida location. You have to scrape company_name, No. of days ago when job was posted, Rating of the company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Company name Job post duration Rating\n",
      "0            IHS Markit                3d      4\n",
      "1               Genpact               23d    3.8\n",
      "2              Brickred               14d    3.7\n",
      "3        Biz2Credit Inc                3d    3.7\n",
      "4    Ericsson-Worldwide               17d      4\n",
      "5            ESRI, Inc.                3d    3.7\n",
      "6                Amazon               10d    4.3\n",
      "7  MARKTECH CONSULTANCY               24h       \n",
      "8              Techlive                1d      5\n",
      "9             Algoscale               23d    3.7\n"
     ]
    }
   ],
   "source": [
    "driver=webdriver.Chrome(executable_path='D:\\chromedriver.exe')\n",
    "# Function Definition\n",
    "def data_sci(url):\n",
    "    company_name, post_duration, company_rating = [], [], []\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    g_in = driver.find_element_by_xpath('/html/body/div[2]/div/div/div/div/div[1]/article/header/nav/div/div/div[4]/div[1]/a')\n",
    "    action = ActionChains(driver) \n",
    "    action.click(on_element=g_in)\n",
    "    action.perform() \n",
    "    time.sleep(3)\n",
    "    \n",
    "    email = driver.find_element_by_xpath(\"/html/body/div[8]/div/div/div[2]/div[2]/div[2]/div/div/div/div[3]/form/div[1]/div/div/input\")\n",
    "    email.send_keys('patarhemant@gmail.com')\n",
    "    time.sleep(3)\n",
    "    \n",
    "    passowrd = driver.find_element_by_xpath(\"/html/body/div[8]/div/div/div[2]/div[2]/div[2]/div/div/div/div[3]/form/div[2]/div/div/input\")\n",
    "    passowrd.send_keys('Fire@123')\n",
    "    time.sleep(3)\n",
    "    \n",
    "    g_in = driver.find_element_by_xpath('/html/body/div[8]/div/div/div[2]/div[2]/div[2]/div/div/div/div[3]/form/div[3]/div[1]/button')\n",
    "    action = ActionChains(driver) \n",
    "    action.click(on_element=g_in)\n",
    "    action.perform() \n",
    "    time.sleep(3)\n",
    "        \n",
    "    driver.find_element_by_xpath(\"//input[@id='sc.keyword']\").send_keys('Data Scientist')\n",
    "    time.sleep(3)\n",
    "    \n",
    "    driver.find_element_by_xpath(\"//input[@id='sc.location']\").send_keys('Noida')\n",
    "    time.sleep(3)\n",
    "\n",
    "    driver.find_element_by_xpath(\"/html/body/header/nav[1]/div/div/div/div[4]/div[3]/form/div/button/span\").click()\n",
    "    time.sleep(2)\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    jobs = soup.find_all('div', attrs ={'class':'jobHeader d-flex justify-content-between align-items-start'})\n",
    "    for i in jobs:\n",
    "        company_name.append(i.text)\n",
    "        \n",
    "    jobs1 = soup.find_all('div', attrs ={'class':'d-flex align-items-end pl-std css-mi55ob'})\n",
    "    for j in jobs1:\n",
    "        post_duration.append(j.text)\n",
    "        \n",
    "    jobs2 = soup.find_all('div', attrs ={'class':'d-flex flex-column css-fbt9gv e1rrn5ka2'})\n",
    "    for k in jobs2:\n",
    "        company_rating.append(k.text)\n",
    "            \n",
    "    df=pd.DataFrame({'Company name':company_name[:10],\n",
    "                     'Job post duration':post_duration[:10],\n",
    "                    'Rating':company_rating[:10]})\n",
    "    print(df)\n",
    "    df.to_csv('glassdoor_data_scientist_10.csv', index = False)\n",
    "#     print(company_name)\n",
    "#     print(post_duration)\n",
    "#     print(company_rating)\n",
    "    \n",
    "# Calling Function\n",
    "try:\n",
    "    data_sci(\"https://www.glassdoor.co.in/index.htm\")\n",
    "    driver.quit()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q5: Q5: Write a python program to scrape the salary data for Data Scientist designation in Noida location webpage https://www.glassdoor.co.in/Salaries/index.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Company name salary count Avg Salary Min Salary  \\\n",
      "0                       Delhivery           13  12,83,026       705K   \n",
      "1                       Accenture           32  11,19,272       571K   \n",
      "2                             IBM           62   7,52,445       580K   \n",
      "3              Ericsson-Worldwide           18   8,28,000       468K   \n",
      "4              UnitedHealth Group           12  13,21,601       708K   \n",
      "5                Analytics Vidhya            8  20,889/mo        14K   \n",
      "6       Tata Consultancy Services           69   6,33,432       488K   \n",
      "7  Cognizant Technology Solutions           40   9,96,446       784K   \n",
      "8              Valiance Solutions           10   7,71,320       496K   \n",
      "9              Vidooly Media Tech           10  12,669/mo         8K   \n",
      "\n",
      "  Max Salary  \n",
      "0    11,495K  \n",
      "1     2,200K  \n",
      "2     2,700K  \n",
      "3     1,595K  \n",
      "4     1,557K  \n",
      "5        22K  \n",
      "6     1,000K  \n",
      "7     1,250K  \n",
      "8     1,138K  \n",
      "9        20K  \n"
     ]
    }
   ],
   "source": [
    "driver=webdriver.Chrome(executable_path='D:\\chromedriver.exe')\n",
    "# Function Definition\n",
    "def data_sci(url):\n",
    "    avg_sal, min_sal, max_sal, sal_cnt, comp = [], [], [], [], []\n",
    "    \n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "            \n",
    "    driver.find_element_by_xpath(\"//input[@id='KeywordSearch']\").send_keys('Data Scientist')\n",
    "    time.sleep(3)\n",
    "    \n",
    "    driver.find_element_by_xpath(\"//input[@id='LocationSearch']\").clear()\n",
    "    time.sleep(3)\n",
    "    \n",
    "    driver.find_element_by_xpath(\"//input[@id='LocationSearch']\").send_keys('Noida')\n",
    "    time.sleep(3)\n",
    "\n",
    "    driver.find_element_by_xpath(\"/html/body/div[3]/div/div[1]/div[1]/div/div/form/button\").click()\n",
    "    time.sleep(2)\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "   \n",
    "    jobs = soup.find_all('div', attrs ={'data-test':'salary-list-items'})\n",
    "\n",
    "    avg_sal = [str(i.text).split()[-1].split(\"₹\")[0].rstrip(\"/yr\") for i in jobs]\n",
    "    min_sal = [str(i.text).split()[-1].split(\"₹\")[1] for i in jobs]\n",
    "    max_sal = [str(i.text).split()[-1].split(\"₹\")[2] for i in jobs]\n",
    "    sal_cnt = [str(i.text).split(\"salaries\")[1].lstrip(\"See \").rstrip(\" \") for i in jobs]\n",
    "    comp = [str(i.text).split(\"₹\")[0].split(\" Scientist\")[1].replace(\"- Monthly Intern\",\"\") for i in jobs]\n",
    "\n",
    "#     print(avg_sal)\n",
    "#     print(min_sal)\n",
    "#     print(max_sal)\n",
    "#     print(sal_cnt)\n",
    "#     print(comp)\n",
    "     \n",
    "    df=pd.DataFrame({'Company name':comp[:10],\n",
    "                     'salary count':sal_cnt[:10],\n",
    "                    'Avg Salary':avg_sal[:10],\n",
    "                     'Min Salary':min_sal[:10],\n",
    "                    'Max Salary':max_sal[:10]})\n",
    "    print(df)\n",
    "    df.to_csv('glassdoor_data_scientist_salary_10.csv', index = False)\n",
    "\n",
    "    \n",
    "# Calling Function\n",
    "try:\n",
    "    data_sci(\"https://www.glassdoor.co.in/Salaries/index.htm\")\n",
    "    driver.quit()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q6 : Scrape data of first 100 sunglasses listings on flipkart.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Brand                                Product description Price  \\\n",
      "0     Silver Kartz         UV Protection Retro Square Sunglasses (56)   211   \n",
      "1     Silver Kartz              UV Protection Aviator Sunglasses (88)   228   \n",
      "2   FDA COLLECTION  Gradient, Mirrored, UV Protection Round, Round...   199   \n",
      "3       Phenomenal  UV Protection, Mirrored Retro Square Sunglasse...   399   \n",
      "4   Rozzetta Craft   UV Protection Rectangular Sunglasses (Free Size)   398   \n",
      "..             ...                                                ...   ...   \n",
      "95        DAHAAZIL                     Gradient Round Sunglasses (55)   149   \n",
      "96       Elligator              UV Protection Aviator Sunglasses (55)   332   \n",
      "97        Fastrack             UV Protection Wayfarer Sunglasses (53)   751   \n",
      "98        Fastrack   UV Protection Rectangular Sunglasses (Free Size)   687   \n",
      "99           Orkee  Mirrored Rectangular, Aviator, Clubmaster Sung...   272   \n",
      "\n",
      "   Discount  \n",
      "0        85  \n",
      "1        80  \n",
      "2        84  \n",
      "3        80  \n",
      "4        73  \n",
      "..      ...  \n",
      "95       70  \n",
      "96       86  \n",
      "97       16  \n",
      "98       14  \n",
      "99       80  \n",
      "\n",
      "[100 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "driver=webdriver.Chrome(executable_path='D:\\chromedriver.exe')\n",
    "# Function Definition\n",
    "def data_sci(url):\n",
    "    brand, pro_d, pri, dis = [], [], [], []\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    driver.find_element_by_xpath(\"/html/body/div/div/div[1]/div[1]/div[2]/div[2]/form/div/div/input\").send_keys('sunglasses')\n",
    "    time.sleep(3)\n",
    "    \n",
    "    driver.find_element_by_xpath(\"/html/body/div[2]/div/div/button\").click()\n",
    "    time.sleep(2)\n",
    "    \n",
    "    driver.find_element_by_xpath(\"/html/body/div/div/div[1]/div[1]/div[2]/div[2]/form/div/button\").click()\n",
    "    time.sleep(3)\n",
    "    \n",
    "    for g in range(3):\n",
    "        g = g + 1\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        jobs = soup.find_all('div', attrs ={'class':'_2WkVRV'})\n",
    "        for j in jobs:\n",
    "            brand.append(j.text)\n",
    "        jobs2 = soup.find_all('a', attrs ={'class':'IRpwTa'})\n",
    "        for k in jobs2:\n",
    "            pro_d.append(k.text)\n",
    "            \n",
    "        jobs1 = soup.find_all('a', attrs ={'class':'_3bPFwb'})\n",
    "        for i in jobs1:\n",
    "            pri1 = str(i.text).split(\"₹\")[1] \n",
    "            pri.append(pri1)\n",
    "            \n",
    "        try:\n",
    "            for i in jobs1:\n",
    "                dis1 = str(((i.text).split(\"₹\")[2]).split(\"%\")[0])[-2:]\n",
    "                dis.append(dis1)\n",
    "        except NoSuchElementException:\n",
    "            dis.append('-')\n",
    "\n",
    "        try:\n",
    "            driver.find_element_by_xpath(\"/html/body/div[1]/div/div[3]/div[2]/div[1]/div[2]/div[12]/div/div/nav/a[\"+str(g+10)+\"]\").click()\n",
    "            time.sleep(5)\n",
    "        except NoSuchElementException:\n",
    "            pass\n",
    "             \n",
    "#     print(len(brand))\n",
    "#     print(len(pro_d))\n",
    "#     print(len(pri))\n",
    "#     print(len(dis))     \n",
    "#     print(brand)\n",
    "#     print(pro_d)\n",
    "#     print(pri)\n",
    "#     print(dis)  \n",
    "    \n",
    "    df=pd.DataFrame({'Brand':brand[:100],\n",
    "                     'Product description':pro_d[:100],\n",
    "                    'Price':pri[:100],\n",
    "                     'Discount':dis[:100]})\n",
    "    print(df)\n",
    "    df.to_csv('flipkart_sunglass_100.csv', index = False)\n",
    "    \n",
    "# Calling Function\n",
    "try:\n",
    "    data_sci(\"https://www.flipkart.com/\")\n",
    "    driver.quit()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q7: Scrape 100 reviews data from flipkart.com for iphone11 phone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n",
      "100\n",
      "   Rating      Review Summery  \\\n",
      "0       5    Perfect product!   \n",
      "1       5       Great product   \n",
      "2       5    Perfect product!   \n",
      "3       5  Highly recommended   \n",
      "4       5    Perfect product!   \n",
      "..    ...                 ...   \n",
      "95      5   Worth every penny   \n",
      "96      5   Worth every penny   \n",
      "97      5    Perfect product!   \n",
      "98      4        Nice product   \n",
      "99      5           Wonderful   \n",
      "\n",
      "                                               Review  \n",
      "0   Amazing phone with great cameras and better ba...  \n",
      "1   Amazing Powerful and Durable Gadget.I’m am ver...  \n",
      "2   It’s a must buy who is looking for an upgrade ...  \n",
      "3   iphone 11 is a very good phone to buy only if ...  \n",
      "4   Value for money❤️❤️Its awesome mobile phone in...  \n",
      "..                                                ...  \n",
      "95  Best budget Iphone till date ❤️ go for it guys...  \n",
      "96  It’s been almost a month since I have been usi...  \n",
      "97  Iphone is just awesome.. battery backup is ver...  \n",
      "98  Awesome Phone. Slightly high price but worth. ...  \n",
      "99  *Review after 10 months of usage*Doesn't seem ...  \n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "driver=webdriver.Chrome(executable_path='D:\\chromedriver.exe')\n",
    "# Function Definition\n",
    "def data_sci(url):\n",
    "    rating, rev_s, rev = [], [], []\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    driver.find_element_by_xpath(\"/html/body/div[1]/div/div[3]/div[1]/div[2]/div[9]/div/div/div[5]/div/a/div\").click()\n",
    "    time.sleep(3)\n",
    "    \n",
    "    for g in range(10):\n",
    "        g = g + 1\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        phn = soup.find_all('div', attrs ={'class':'_27M-vq'})\n",
    "        for j in phn:\n",
    "            rate = str(j.text)[:1]   \n",
    "            rating.append(rate)\n",
    "        phn1 = soup.find_all('p', attrs ={'class':'_2-N8zT'})\n",
    "        for k in phn1:            \n",
    "            rev_s.append(k.text)\n",
    "        phn2 = soup.find_all('div', attrs ={'class':'t-ZTKy'})\n",
    "        for l in phn2:\n",
    "            revw = (l.text).split(\"READ MORE\")[0]\n",
    "            rev.append(revw)\n",
    "\n",
    "        if g == 1:\n",
    "            driver.find_element_by_xpath(\"/html/body/div[1]/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[11]\").click()\n",
    "            time.sleep(5)\n",
    "        elif g == range(2,5):\n",
    "            driver.find_element_by_xpath(\"/html/body/div[1]/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[\"+str(g+2)+\"]\").click()\n",
    "            time.sleep(5)\n",
    "        else:\n",
    "            url1 = 'https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/product-reviews/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace=FLIPKART&page='\n",
    "            page = requests.get(url1+str(g+1))\n",
    "\n",
    "    print(len(rating))\n",
    "    print(len(rev_s))\n",
    "    print(len(rev))     \n",
    "#     print(rating)\n",
    "#     print(rev_s)\n",
    "#     print(rev)\n",
    "    \n",
    "    df=pd.DataFrame({'Rating':rating[:100],\n",
    "                    'Review Summery':rev_s[:100],\n",
    "                     'Review':rev[:100]})\n",
    "    print(df)\n",
    "    df.to_csv('flipkart_iphone_revw_100.csv', index = False)\n",
    "    \n",
    "# Calling Function\n",
    "try:\n",
    "    data_sci(\"https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace.\")\n",
    "    driver.quit()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q8: Scrape data for first 100 sneakers you find when you visit flipkart.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Brand                                Product description  \\\n",
      "0                Chevit                 Super 445 Fashion Sneakers For Men   \n",
      "1         WHITE WALKERS                                   Sneakers For Men   \n",
      "2                Chevit  Combo Pack of 4 Casual Sneakers With Sneakers ...   \n",
      "3                Chevit  171 Smart Tan Lace-Ups Casuals for Men Sneaker...   \n",
      "4            Shoes Bank     White Sneaker For Men's/Boy's Sneakers For Men   \n",
      "..                  ...                                                ...   \n",
      "95  World Wear Footwear  Combo Pack of 2 Latest Collection Stylish casu...   \n",
      "96             ASTEROID  Feather Print New Spring Season White Shoes Me...   \n",
      "97              Edoeviv                                   Sneakers For Men   \n",
      "98                 Aura                      Casual Shoes Sneakers For Men   \n",
      "99                 Puma  Puma Smash v2 L Perf Puma White-Puma Whi Sneak...   \n",
      "\n",
      "    Price Discount  \n",
      "0     369       26  \n",
      "1     499       50  \n",
      "2     579       70  \n",
      "3     249       50  \n",
      "4     398       60  \n",
      "..    ...      ...  \n",
      "95    499       50  \n",
      "96    500       49  \n",
      "97    398       43  \n",
      "98    367       26  \n",
      "99  1,599       60  \n",
      "\n",
      "[100 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "driver=webdriver.Chrome(executable_path='D:\\chromedriver.exe')\n",
    "# Function Definition\n",
    "def data_sci(url):\n",
    "    brand, pro_d, pri, dis = [], [], [], []\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    driver.find_element_by_xpath(\"/html/body/div/div/div[1]/div[1]/div[2]/div[2]/form/div/div/input\").send_keys('sneakers')\n",
    "    time.sleep(3)\n",
    "    \n",
    "    driver.find_element_by_xpath(\"/html/body/div[2]/div/div/button\").click()\n",
    "    time.sleep(2)\n",
    "    \n",
    "    driver.find_element_by_xpath(\"/html/body/div/div/div[1]/div[1]/div[2]/div[2]/form/div/button\").click()\n",
    "    time.sleep(3)\n",
    "    \n",
    "    for g in range(3):\n",
    "        g = g + 1\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        jobs = soup.find_all('div', attrs ={'class':'_2WkVRV'})\n",
    "        for j in jobs:\n",
    "            brand.append(j.text)\n",
    "        jobs2 = soup.find_all('a', attrs ={'class':'IRpwTa'})\n",
    "        for k in jobs2:\n",
    "            pro_d.append(k.text)\n",
    "            \n",
    "        jobs1 = soup.find_all('a', attrs ={'class':'_3bPFwb'})\n",
    "        for i in jobs1:          \n",
    "            pri1 = str(i.text).split(\"₹\")[1] \n",
    "            pri.append(pri1)\n",
    "            \n",
    "        for i in jobs1:\n",
    "            try:\n",
    "                dis1 = str(((i.text).split(\"₹\")[2]).split(\"%\")[0])[-2:]\n",
    "                dis.append(dis1)\n",
    "            except:\n",
    "                dis.append('-')\n",
    "\n",
    "        try:\n",
    "            driver.find_element_by_xpath(\"/html/body/div[1]/div/div[3]/div[2]/div[1]/div[2]/div[12]/div/div/nav/a[\"+str(g+10)+\"]\").click()\n",
    "            time.sleep(5)\n",
    "        except NoSuchElementException:\n",
    "            pass\n",
    "             \n",
    "#     print(len(brand))\n",
    "#     print(len(pro_d))\n",
    "#     print(len(pri))\n",
    "#     print(len(dis))     \n",
    "#     print(brand)\n",
    "#     print(pro_d)\n",
    "#     print(pri)\n",
    "#     print(dis)  \n",
    "    \n",
    "    df=pd.DataFrame({'Brand':brand[:100],\n",
    "                     'Product description':pro_d[:100],\n",
    "                    'Price':pri[:100],\n",
    "                     'Discount':dis[:100]})\n",
    "    print(df)\n",
    "    df.to_csv('flipkart_sneakers_100.csv', index = False)\n",
    "    \n",
    "# Calling Function\n",
    "try:\n",
    "    data_sci(\"https://www.flipkart.com/\")\n",
    "    driver.quit()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q9: Go to the link - https://www.myntra.com/shoes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n",
      "100\n",
      "               Brand                          Description  Price\n",
      "0               Nike        Men FREE RN 5.0 Running Shoes   7995\n",
      "1   ADIDAS Originals                Men POD-S3.1 Sneakers  11999\n",
      "2               ALDO         Men Textured Leather Loafers  12999\n",
      "3          Cole Haan  Men Traveller Leather Penny Loafers   9999\n",
      "4               Geox            Men Leather Formal Derbys  11490\n",
      "..               ...                                  ...    ...\n",
      "95              FILA         Heritage Stackhouse Sneakers   8999\n",
      "96      UNDER ARMOUR       Women HOVR Rise Training Shoes   8499\n",
      "97              Geox             Women Leather Flat Boots  12999\n",
      "98  ADIDAS Originals                Men LXCON 94 Sneakers  11999\n",
      "99         Cole Haan          Women Woven Design Sneakers  11249\n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "driver=webdriver.Chrome(executable_path='D:\\chromedriver.exe')\n",
    "# Function Definition\n",
    "def data_sci(url):\n",
    "    brand, dec, pri = [], [], []\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    p_cb = driver.find_element_by_xpath('/html/body/div[2]/div/div[1]/main/div[3]/div[1]/section/div/div[5]/ul/li[2]/label/div')\n",
    "    action = ActionChains(driver) \n",
    "    action.click(on_element=p_cb)\n",
    "    action.perform() \n",
    "    time.sleep(3)\n",
    "    \n",
    "    c_cb = driver.find_element_by_xpath('/html/body/div[2]/div/div[1]/main/div[3]/div[1]/section/div/div[6]/ul/li[1]/label/div')\n",
    "    action = ActionChains(driver) \n",
    "    action.click(on_element=c_cb) \n",
    "    action.perform() \n",
    "    time.sleep(3)\n",
    "    \n",
    "    for g in range(2):\n",
    "        g = g + 1\n",
    "    \n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        jobs = soup.find_all('h3', attrs ={'class':'product-brand'})\n",
    "        for i in jobs:\n",
    "            brand.append(i.text)\n",
    "\n",
    "        jobs1 = soup.find_all('h4', attrs ={'class':'product-product'})\n",
    "        for j in jobs1:\n",
    "            dec.append(j.text)\n",
    "\n",
    "        jobs2 = soup.find_all('div', attrs ={'class':'product-price'})\n",
    "        for k in jobs2:\n",
    "            pri.append((k.text).split(\"Rs. \")[1])\n",
    "\n",
    "        driver.find_element_by_xpath(\"/html/body/div[2]/div/div[1]/main/div[3]/div[2]/div/div[2]/section/div[2]/ul/li[3]/a\").click()\n",
    "        time.sleep(5)\n",
    "                          \n",
    "    print(len(brand))\n",
    "    print(len(dec))\n",
    "    print(len(pri))\n",
    "     \n",
    "#     print(brand)\n",
    "#     print(dec)\n",
    "#     print(pri)\n",
    "    \n",
    "    df=pd.DataFrame({'Brand':brand[:100],\n",
    "                     'Description':dec[:100],\n",
    "                    'Price':pri[:100]})\n",
    "    print(df)\n",
    "    df.to_csv('mintra_shoes_filtered_100.csv', index = False)\n",
    "\n",
    "    \n",
    "# Calling Function\n",
    "try:\n",
    "    data_sci(\"https://www.myntra.com/shoes\")\n",
    "    driver.quit()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q10: Go to webpage https://www.amazon.in/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
