{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEB SCRAPING ASSIGNMENT 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.webdriver.common.action_chains import ActionChains \n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1. Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. Scrape the job-title, job-location, company_name, experience_required. Scrape first 10 jobs data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Definition\n",
    "def data_sci(url):\n",
    "    job_desp, company, location, experience = [], [], [], []\n",
    "    driver=webdriver.Chrome(r'D:\\chromedriver.exe')\n",
    "    driver.get(url)\n",
    "    search_job = driver.find_element_by_xpath(\"//input[@id='qsb-keyword-sugg']\")\n",
    "    search_job.send_keys('Data Analyst')\n",
    "    search_loc = driver.find_element_by_xpath(\"//input[@id='qsb-location-sugg']\")\n",
    "    search_loc.send_keys('Bangalore')\n",
    "    search_button=driver.find_element_by_xpath(\"//div[@class='search-btn']/button\")\n",
    "    search_button.click()\n",
    "    time.sleep(5)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    jobs = soup.find_all('div', attrs ={'class':'info fleft'})\n",
    "    for x, i in enumerate(jobs):\n",
    "        for y, j in enumerate(i.find_all('a')):\n",
    "            if \"Reviews\" in str(j.text): pass\n",
    "            else:\n",
    "                if y+1 == 1: \n",
    "                    job_desp.append(j.text)\n",
    "                else: company.append(j.text)\n",
    "        for z, k in enumerate(i.find('ul', attrs = {\"class\": \"mt-7\"}).find_all(\"li\")):\n",
    "            if z+1 == 2: pass\n",
    "            else: \n",
    "                if z+1 == 1: experience.append(k.text)\n",
    "                else: location.append(k.text)\n",
    "    driver.quit()\n",
    "    df=pd.DataFrame({'Job Title':job_desp[:10],\n",
    "                     'Company':company[:10],\n",
    "                     'Experiance':experience[:10],\n",
    "                    'Location':location[:10]})\n",
    "    print(df)\n",
    "    df.to_csv('naukri_data_analyst_10.csv', index = False)\n",
    "\n",
    "#     print(job_desp)\n",
    "#     print(company)\n",
    "#     print(experience)\n",
    "#     print(location)\n",
    "\n",
    "# Calling Function\n",
    "data_sci(\"https://www.naukri.com/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, full job-description. You have to scrape first 10 jobs data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           Job Title  \\\n",
      "0  Fresher  Data Engineer / Data Scientist / Data...   \n",
      "1  Immediate opening For Data Scientist/Data Analyst   \n",
      "2                      Data Scientist / Data Analyst   \n",
      "3             Senior Data Scientist - NLP/ Python/ R   \n",
      "4           Explore job openings on Data Scientist!!   \n",
      "5  Lead Data Scientist - Machine Learning/ Data M...   \n",
      "6                              Senior Data Scientist   \n",
      "7                  Data Scientist - Machine Learning   \n",
      "8     Artificial Intelligence Analyst/Data Scientist   \n",
      "9                                     Data Scientist   \n",
      "\n",
      "                                             Company  \\\n",
      "0                     ACHYUTAS SOFT PRIVATE LIMITED    \n",
      "1  CAIA-Center For Artificial Intelligence & Adva...   \n",
      "2                          Altimetrik India Pvt. Ltd   \n",
      "3                                 AVI Consulting LLP   \n",
      "4                          Bristlecone India Limited   \n",
      "5                       Wrackle Technologies Pvt Ltd   \n",
      "6                                            Signify   \n",
      "7                  BLUE YONDER INDIA PRIVATE LIMITED   \n",
      "8                            TalentCo Search Pvt Ltd   \n",
      "9                            Mailkit Private Limited   \n",
      "\n",
      "                                         Description  \\\n",
      "0  Roles and ResponsibilitiesAnalytical Skills:To...   \n",
      "1   Dear Candidate  Schedule a Telephonic Intervi...   \n",
      "2      3+ years of data science,         data ana...   \n",
      "3  Roles and ResponsibilitiesSkill : NLP,Semantic...   \n",
      "4   JOB DESCRIPTION  Exp  4+ years 1) Hands-on py...   \n",
      "5  Roles and ResponsibilitiesRequirements :- 6-9 ...   \n",
      "6   We re on the lookout for forward-thinking inn...   \n",
      "7  The Yantriks Data Science and Machine Learning...   \n",
      "8  Roles and Responsibilities- Machine Learning t...   \n",
      "9  Mailkit is an European Marketing Automation co...   \n",
      "\n",
      "                                            Location  \n",
      "0                    Delhi NCR, Bengaluru, Hyderabad  \n",
      "1                Chennai, Pune, Bengaluru, Hyderabad  \n",
      "2                                          Bengaluru  \n",
      "3                               Bengaluru, Hyderabad  \n",
      "4                            Pune, Mumbai, Bengaluru  \n",
      "5                                          Bengaluru  \n",
      "6                                          Bengaluru  \n",
      "7                                          Bengaluru  \n",
      "8                                  Mumbai, Bengaluru  \n",
      "9  Chennai, Pune, Mumbai, Bengaluru, Hyderabad, K...  \n"
     ]
    }
   ],
   "source": [
    "# Function Definition\n",
    "def data_sci(url):\n",
    "    job_desp, company, location, description, urls = [], [], [], [],[]\n",
    "    driver=webdriver.Chrome(r'D:\\chromedriver.exe')\n",
    "    driver.get(url)\n",
    "    search_job = driver.find_element_by_xpath(\"//input[@id='qsb-keyword-sugg']\")\n",
    "    search_job.send_keys('Data Scientist')\n",
    "    search_loc = driver.find_element_by_xpath(\"//input[@id='qsb-location-sugg']\")\n",
    "    search_loc.send_keys('Bangalore')\n",
    "    search_button=driver.find_element_by_xpath(\"//div[@class='search-btn']/button\")\n",
    "    search_button.click()\n",
    "    time.sleep(5)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    jobs = soup.find_all('div', attrs ={'class':'info fleft'})\n",
    "\n",
    "    for x, i in enumerate(jobs):\n",
    "        for y, j in enumerate(i.find_all('a')):\n",
    "            if \"Reviews\" in str(j.text): pass\n",
    "            else:\n",
    "                if y+1 == 1: \n",
    "                    job_desp.append(j.text)\n",
    "                    urls.append(j.get('href'))\n",
    "                else: company.append(j.text)\n",
    "        for z, k in enumerate(i.find('ul', attrs = {\"class\": \"mt-7\"}).find_all(\"li\")):\n",
    "            if z+1 == 3: location.append(k.text)\n",
    "            else: pass\n",
    "    \n",
    "    for url in urls:\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        soup1 = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        jobs_d = soup1.find('div', attrs ={'class':['dang-inner-html','clearboth description']})\n",
    "        description.append(jobs_d.text)\n",
    "    \n",
    "    driver.close()\n",
    "    driver.quit()\n",
    "    df=pd.DataFrame({'Job Title':job_desp[:10],\n",
    "                     'Company':company[:10],\n",
    "                     'Description':description[:10],\n",
    "                    'Location':location[:10]})\n",
    "    print(df)\n",
    "    df.to_csv('naukri_data_scientist_10.csv', index = False)\n",
    "#     print(job_desp)\n",
    "#     print(company)\n",
    "#     print(description)\n",
    "#     print(location)\n",
    "#     print(urls)\n",
    "\n",
    "# Calling Function\n",
    "data_sci(\"https://www.naukri.com/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3: webpage https://www.naukri.com/ scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           Job Title  \\\n",
      "0  Fresher  Data Engineer / Data Scientist / Data...   \n",
      "1           Data Scientist - Python/Machine Learning   \n",
      "2                                     Data Scientist   \n",
      "3  Only Fresher / Data Scientist / Data Analyst /...   \n",
      "4                                     Data Scientist   \n",
      "5                                     Data Scientist   \n",
      "6                                Lead Data Scientist   \n",
      "7  Excellent opportunity For Lead Data Scientist ...   \n",
      "8                                Lead Data Scientist   \n",
      "9  Excellent opportunity For Lead Data Scientist ...   \n",
      "\n",
      "                             Company Experiance  \\\n",
      "0     ACHYUTAS SOFT PRIVATE LIMITED     0-2 Yrs   \n",
      "1                              Jubna    5-8 Yrs   \n",
      "2             IBM India Pvt. Limited    4-8 Yrs   \n",
      "3         GABA Consultancy services     0-0 Yrs   \n",
      "4              PureSoftware Pvt Ltd.    5-9 Yrs   \n",
      "5             Air Asia India Limited    1-4 Yrs   \n",
      "6  NEC CORPORATION INDIA PRIVATE LTD   9-14 Yrs   \n",
      "7  NEC CORPORATION INDIA PRIVATE LTD   9-14 Yrs   \n",
      "8  NEC CORPORATION INDIA PRIVATE LTD   9-14 Yrs   \n",
      "9  NEC CORPORATION INDIA PRIVATE LTD   9-14 Yrs   \n",
      "\n",
      "                             Location  \n",
      "0     Delhi NCR, Bengaluru, Hyderabad  \n",
      "1                               Noida  \n",
      "2                    Gurgaon Gurugram  \n",
      "3     Delhi NCR, Greater Noida, Noida  \n",
      "4                             Gurgaon  \n",
      "5                  Delhi NCR, Gurgaon  \n",
      "6  Delhi NCR, Noida(Sector-142 Noida)  \n",
      "7  Delhi NCR(Sector-142 Noida), Noida  \n",
      "8  Delhi NCR, Noida(Sector-142 Noida)  \n",
      "9  Delhi NCR(Sector-142 Noida), Noida  \n"
     ]
    }
   ],
   "source": [
    "driver=webdriver.Chrome(executable_path='D:\\chromedriver.exe')\n",
    "# Function Definition\n",
    "def data_sci(url):\n",
    "    job_desp, company, location, experience = [], [], [], []\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "\n",
    "    driver.find_element_by_xpath(\"//input[@id='qsb-keyword-sugg']\").send_keys('Data Scientist')\n",
    "    time.sleep(3)\n",
    "\n",
    "    driver.find_element_by_xpath(\"//div[@class='search-btn']/button\").click()\n",
    "    time.sleep(2)\n",
    "    \n",
    "    j_loc = driver.find_element_by_xpath('/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[2]/div[2]/div[2]/label/i')\n",
    "    action = ActionChains(driver) \n",
    "    action.click(on_element=j_loc)\n",
    "    action.perform() \n",
    "    time.sleep(3)\n",
    "    \n",
    "    j_sal = driver.find_element_by_xpath('/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[3]/div[2]/div[2]/label/i')\n",
    "    action = ActionChains(driver) \n",
    "    action.click(on_element=j_sal) \n",
    "    action.perform() \n",
    "    time.sleep(3)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    jobs = soup.find_all('div', attrs ={'class':'info fleft'})\n",
    "    for x, i in enumerate(jobs):\n",
    "        for y, j in enumerate(i.find_all('a')):\n",
    "            if \"Reviews\" in str(j.text): pass\n",
    "            else:\n",
    "                if y+1 == 1: \n",
    "                    job_desp.append(j.text)\n",
    "                else: company.append(j.text)\n",
    "        for z, k in enumerate(i.find('ul', attrs = {\"class\": \"mt-7\"}).find_all(\"li\")):\n",
    "            if z+1 == 2: pass\n",
    "            else: \n",
    "                if z+1 == 1: experience.append(k.text)\n",
    "                else: location.append(k.text)\n",
    "    \n",
    "    df=pd.DataFrame({'Job Title':job_desp[:10],\n",
    "                     'Company':company[:10],\n",
    "                     'Experiance':experience[:10],\n",
    "                    'Location':location[:10]})\n",
    "    print(df)\n",
    "    df.to_csv('naukri_data_scientist_filtered_10.csv', index = False)\n",
    "\n",
    "# Calling Function\n",
    "try:\n",
    "    data_sci(\"https://www.naukri.com/\")\n",
    "    driver.quit()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4: Write a python program to scrape data for first 10 job results for Data scientist Designation in Noida location. You have to scrape company_name, No. of days ago when job was posted, Rating of the company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Company name Job post duration Rating\n",
      "0            IHS Markit                3d      4\n",
      "1               Genpact               23d    3.8\n",
      "2              Brickred               14d    3.7\n",
      "3        Biz2Credit Inc                3d    3.7\n",
      "4    Ericsson-Worldwide               17d      4\n",
      "5            ESRI, Inc.                3d    3.7\n",
      "6                Amazon               10d    4.3\n",
      "7  MARKTECH CONSULTANCY               24h       \n",
      "8              Techlive                1d      5\n",
      "9             Algoscale               23d    3.7\n"
     ]
    }
   ],
   "source": [
    "driver=webdriver.Chrome(executable_path='D:\\chromedriver.exe')\n",
    "# Function Definition\n",
    "def data_sci(url):\n",
    "    company_name, post_duration, company_rating = [], [], []\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    g_in = driver.find_element_by_xpath('/html/body/div[2]/div/div/div/div/div[1]/article/header/nav/div/div/div[4]/div[1]/a')\n",
    "    action = ActionChains(driver) \n",
    "    action.click(on_element=g_in)\n",
    "    action.perform() \n",
    "    time.sleep(3)\n",
    "    \n",
    "    email = driver.find_element_by_xpath(\"/html/body/div[8]/div/div/div[2]/div[2]/div[2]/div/div/div/div[3]/form/div[1]/div/div/input\")\n",
    "    email.send_keys('patarhemant@gmail.com')\n",
    "    time.sleep(3)\n",
    "    \n",
    "    passowrd = driver.find_element_by_xpath(\"/html/body/div[8]/div/div/div[2]/div[2]/div[2]/div/div/div/div[3]/form/div[2]/div/div/input\")\n",
    "    passowrd.send_keys('Fire@123')\n",
    "    time.sleep(3)\n",
    "    \n",
    "    g_in = driver.find_element_by_xpath('/html/body/div[8]/div/div/div[2]/div[2]/div[2]/div/div/div/div[3]/form/div[3]/div[1]/button')\n",
    "    action = ActionChains(driver) \n",
    "    action.click(on_element=g_in)\n",
    "    action.perform() \n",
    "    time.sleep(3)\n",
    "        \n",
    "    driver.find_element_by_xpath(\"//input[@id='sc.keyword']\").send_keys('Data Scientist')\n",
    "    time.sleep(3)\n",
    "    \n",
    "    driver.find_element_by_xpath(\"//input[@id='sc.location']\").send_keys('Noida')\n",
    "    time.sleep(3)\n",
    "\n",
    "    driver.find_element_by_xpath(\"/html/body/header/nav[1]/div/div/div/div[4]/div[3]/form/div/button/span\").click()\n",
    "    time.sleep(2)\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    jobs = soup.find_all('div', attrs ={'class':'jobHeader d-flex justify-content-between align-items-start'})\n",
    "    for i in jobs:\n",
    "        company_name.append(i.text)\n",
    "        \n",
    "    jobs1 = soup.find_all('div', attrs ={'class':'d-flex align-items-end pl-std css-mi55ob'})\n",
    "    for j in jobs1:\n",
    "        post_duration.append(j.text)\n",
    "        \n",
    "    jobs2 = soup.find_all('div', attrs ={'class':'d-flex flex-column css-fbt9gv e1rrn5ka2'})\n",
    "    for k in jobs2:\n",
    "        company_rating.append(k.text)\n",
    "            \n",
    "    df=pd.DataFrame({'Company name':company_name[:10],\n",
    "                     'Job post duration':post_duration[:10],\n",
    "                    'Rating':company_rating[:10]})\n",
    "    print(df)\n",
    "    df.to_csv('glassdoor_data_scientist_10.csv', index = False)\n",
    "#     print(company_name)\n",
    "#     print(post_duration)\n",
    "#     print(company_rating)\n",
    "    \n",
    "# Calling Function\n",
    "try:\n",
    "    data_sci(\"https://www.glassdoor.co.in/index.htm\")\n",
    "    driver.quit()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q5: Q5: Write a python program to scrape the salary data for Data Scientist designation in Noida location webpage https://www.glassdoor.co.in/Salaries/index.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Company name salary count Avg Salary Min Salary  \\\n",
      "0                       Delhivery           13  12,83,026       705K   \n",
      "1                       Accenture           32  11,19,272       571K   \n",
      "2                             IBM           62   7,52,445       580K   \n",
      "3              Ericsson-Worldwide           18   8,28,000       468K   \n",
      "4              UnitedHealth Group           12  13,21,601       708K   \n",
      "5                Analytics Vidhya            8  20,889/mo        14K   \n",
      "6       Tata Consultancy Services           69   6,33,432       488K   \n",
      "7  Cognizant Technology Solutions           40   9,96,446       784K   \n",
      "8              Valiance Solutions           10   7,71,320       496K   \n",
      "9              Vidooly Media Tech           10  12,669/mo         8K   \n",
      "\n",
      "  Max Salary  \n",
      "0    11,495K  \n",
      "1     2,200K  \n",
      "2     2,700K  \n",
      "3     1,595K  \n",
      "4     1,557K  \n",
      "5        22K  \n",
      "6     1,000K  \n",
      "7     1,250K  \n",
      "8     1,138K  \n",
      "9        20K  \n"
     ]
    }
   ],
   "source": [
    "driver=webdriver.Chrome(executable_path='D:\\chromedriver.exe')\n",
    "# Function Definition\n",
    "def data_sci(url):\n",
    "    avg_sal, min_sal, max_sal, sal_cnt, comp = [], [], [], [], []\n",
    "    \n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "            \n",
    "    driver.find_element_by_xpath(\"//input[@id='KeywordSearch']\").send_keys('Data Scientist')\n",
    "    time.sleep(3)\n",
    "    \n",
    "    driver.find_element_by_xpath(\"//input[@id='LocationSearch']\").clear()\n",
    "    time.sleep(3)\n",
    "    \n",
    "    driver.find_element_by_xpath(\"//input[@id='LocationSearch']\").send_keys('Noida')\n",
    "    time.sleep(3)\n",
    "\n",
    "    driver.find_element_by_xpath(\"/html/body/div[3]/div/div[1]/div[1]/div/div/form/button\").click()\n",
    "    time.sleep(2)\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "   \n",
    "    jobs = soup.find_all('div', attrs ={'data-test':'salary-list-items'})\n",
    "\n",
    "    avg_sal = [str(i.text).split()[-1].split(\"₹\")[0].rstrip(\"/yr\") for i in jobs]\n",
    "    min_sal = [str(i.text).split()[-1].split(\"₹\")[1] for i in jobs]\n",
    "    max_sal = [str(i.text).split()[-1].split(\"₹\")[2] for i in jobs]\n",
    "    sal_cnt = [str(i.text).split(\"salaries\")[1].lstrip(\"See \").rstrip(\" \") for i in jobs]\n",
    "    comp = [str(i.text).split(\"₹\")[0].split(\" Scientist\")[1].replace(\"- Monthly Intern\",\"\") for i in jobs]\n",
    "\n",
    "#     print(avg_sal)\n",
    "#     print(min_sal)\n",
    "#     print(max_sal)\n",
    "#     print(sal_cnt)\n",
    "#     print(comp)\n",
    "     \n",
    "    df=pd.DataFrame({'Company name':comp[:10],\n",
    "                     'salary count':sal_cnt[:10],\n",
    "                    'Avg Salary':avg_sal[:10],\n",
    "                     'Min Salary':min_sal[:10],\n",
    "                    'Max Salary':max_sal[:10]})\n",
    "    print(df)\n",
    "    df.to_csv('glassdoor_data_scientist_salary_10.csv', index = False)\n",
    "\n",
    "    \n",
    "# Calling Function\n",
    "try:\n",
    "    data_sci(\"https://www.glassdoor.co.in/Salaries/index.htm\")\n",
    "    driver.quit()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q6 : Scrape data of first 100 sunglasses listings on flipkart.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Brand                                Product description Price  \\\n",
      "0     Silver Kartz         UV Protection Retro Square Sunglasses (56)   211   \n",
      "1     Silver Kartz              UV Protection Aviator Sunglasses (88)   228   \n",
      "2   FDA COLLECTION  Gradient, Mirrored, UV Protection Round, Round...   199   \n",
      "3       Phenomenal  UV Protection, Mirrored Retro Square Sunglasse...   399   \n",
      "4   Rozzetta Craft   UV Protection Rectangular Sunglasses (Free Size)   398   \n",
      "..             ...                                                ...   ...   \n",
      "95        DAHAAZIL                     Gradient Round Sunglasses (55)   149   \n",
      "96       Elligator              UV Protection Aviator Sunglasses (55)   332   \n",
      "97        Fastrack             UV Protection Wayfarer Sunglasses (53)   751   \n",
      "98        Fastrack   UV Protection Rectangular Sunglasses (Free Size)   687   \n",
      "99           Orkee  Mirrored Rectangular, Aviator, Clubmaster Sung...   272   \n",
      "\n",
      "   Discount  \n",
      "0        85  \n",
      "1        80  \n",
      "2        84  \n",
      "3        80  \n",
      "4        73  \n",
      "..      ...  \n",
      "95       70  \n",
      "96       86  \n",
      "97       16  \n",
      "98       14  \n",
      "99       80  \n",
      "\n",
      "[100 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "driver=webdriver.Chrome(executable_path='D:\\chromedriver.exe')\n",
    "# Function Definition\n",
    "def data_sci(url):\n",
    "    brand, pro_d, pri, dis = [], [], [], []\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    driver.find_element_by_xpath(\"/html/body/div/div/div[1]/div[1]/div[2]/div[2]/form/div/div/input\").send_keys('sunglasses')\n",
    "    time.sleep(3)\n",
    "    \n",
    "    driver.find_element_by_xpath(\"/html/body/div[2]/div/div/button\").click()\n",
    "    time.sleep(2)\n",
    "    \n",
    "    driver.find_element_by_xpath(\"/html/body/div/div/div[1]/div[1]/div[2]/div[2]/form/div/button\").click()\n",
    "    time.sleep(3)\n",
    "    \n",
    "    for g in range(3):\n",
    "        g = g + 1\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        jobs = soup.find_all('div', attrs ={'class':'_2WkVRV'})\n",
    "        for j in jobs:\n",
    "            brand.append(j.text)\n",
    "        jobs2 = soup.find_all('a', attrs ={'class':'IRpwTa'})\n",
    "        for k in jobs2:\n",
    "            pro_d.append(k.text)\n",
    "            \n",
    "        jobs1 = soup.find_all('a', attrs ={'class':'_3bPFwb'})\n",
    "        for i in jobs1:\n",
    "            pri1 = str(i.text).split(\"₹\")[1] \n",
    "            pri.append(pri1)\n",
    "            \n",
    "        try:\n",
    "            for i in jobs1:\n",
    "                dis1 = str(((i.text).split(\"₹\")[2]).split(\"%\")[0])[-2:]\n",
    "                dis.append(dis1)\n",
    "        except NoSuchElementException:\n",
    "            dis.append('-')\n",
    "\n",
    "        try:\n",
    "            driver.find_element_by_xpath(\"/html/body/div[1]/div/div[3]/div[2]/div[1]/div[2]/div[12]/div/div/nav/a[\"+str(g+10)+\"]\").click()\n",
    "            time.sleep(5)\n",
    "        except NoSuchElementException:\n",
    "            pass\n",
    "             \n",
    "#     print(len(brand))\n",
    "#     print(len(pro_d))\n",
    "#     print(len(pri))\n",
    "#     print(len(dis))     \n",
    "#     print(brand)\n",
    "#     print(pro_d)\n",
    "#     print(pri)\n",
    "#     print(dis)  \n",
    "    \n",
    "    df=pd.DataFrame({'Brand':brand[:100],\n",
    "                     'Product description':pro_d[:100],\n",
    "                    'Price':pri[:100],\n",
    "                     'Discount':dis[:100]})\n",
    "    print(df)\n",
    "    df.to_csv('flipkart_sunglass_100.csv', index = False)\n",
    "    \n",
    "# Calling Function\n",
    "try:\n",
    "    data_sci(\"https://www.flipkart.com/\")\n",
    "    driver.quit()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q7: Scrape 100 reviews data from flipkart.com for iphone11 phone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n",
      "100\n",
      "   Rating      Review Summery  \\\n",
      "0       5    Perfect product!   \n",
      "1       5       Great product   \n",
      "2       5    Perfect product!   \n",
      "3       5  Highly recommended   \n",
      "4       5    Perfect product!   \n",
      "..    ...                 ...   \n",
      "95      5   Worth every penny   \n",
      "96      5   Worth every penny   \n",
      "97      5    Perfect product!   \n",
      "98      4        Nice product   \n",
      "99      5           Wonderful   \n",
      "\n",
      "                                               Review  \n",
      "0   Amazing phone with great cameras and better ba...  \n",
      "1   Amazing Powerful and Durable Gadget.I’m am ver...  \n",
      "2   It’s a must buy who is looking for an upgrade ...  \n",
      "3   iphone 11 is a very good phone to buy only if ...  \n",
      "4   Value for money❤️❤️Its awesome mobile phone in...  \n",
      "..                                                ...  \n",
      "95  Best budget Iphone till date ❤️ go for it guys...  \n",
      "96  It’s been almost a month since I have been usi...  \n",
      "97  Iphone is just awesome.. battery backup is ver...  \n",
      "98  Awesome Phone. Slightly high price but worth. ...  \n",
      "99  *Review after 10 months of usage*Doesn't seem ...  \n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "driver=webdriver.Chrome(executable_path='D:\\chromedriver.exe')\n",
    "# Function Definition\n",
    "def data_sci(url):\n",
    "    rating, rev_s, rev = [], [], []\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    driver.find_element_by_xpath(\"/html/body/div[1]/div/div[3]/div[1]/div[2]/div[9]/div/div/div[5]/div/a/div\").click()\n",
    "    time.sleep(3)\n",
    "    \n",
    "    for g in range(10):\n",
    "        g = g + 1\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        phn = soup.find_all('div', attrs ={'class':'_27M-vq'})\n",
    "        for j in phn:\n",
    "            rate = str(j.text)[:1]   \n",
    "            rating.append(rate)\n",
    "        phn1 = soup.find_all('p', attrs ={'class':'_2-N8zT'})\n",
    "        for k in phn1:            \n",
    "            rev_s.append(k.text)\n",
    "        phn2 = soup.find_all('div', attrs ={'class':'t-ZTKy'})\n",
    "        for l in phn2:\n",
    "            revw = (l.text).split(\"READ MORE\")[0]\n",
    "            rev.append(revw)\n",
    "\n",
    "        if g == 1:\n",
    "            driver.find_element_by_xpath(\"/html/body/div[1]/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[11]\").click()\n",
    "            time.sleep(5)\n",
    "        elif g == range(2,5):\n",
    "            driver.find_element_by_xpath(\"/html/body/div[1]/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[\"+str(g+2)+\"]\").click()\n",
    "            time.sleep(5)\n",
    "        else:\n",
    "            url1 = 'https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/product-reviews/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace=FLIPKART&page='\n",
    "            page = requests.get(url1+str(g+1))\n",
    "\n",
    "    print(len(rating))\n",
    "    print(len(rev_s))\n",
    "    print(len(rev))     \n",
    "#     print(rating)\n",
    "#     print(rev_s)\n",
    "#     print(rev)\n",
    "    \n",
    "    df=pd.DataFrame({'Rating':rating[:100],\n",
    "                    'Review Summery':rev_s[:100],\n",
    "                     'Review':rev[:100]})\n",
    "    print(df)\n",
    "    df.to_csv('flipkart_iphone_revw_100.csv', index = False)\n",
    "    \n",
    "# Calling Function\n",
    "try:\n",
    "    data_sci(\"https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace.\")\n",
    "    driver.quit()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q8: Scrape data for first 100 sneakers you find when you visit flipkart.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Brand                                Product description  \\\n",
      "0                Chevit                 Super 445 Fashion Sneakers For Men   \n",
      "1         WHITE WALKERS                                   Sneakers For Men   \n",
      "2                Chevit  Combo Pack of 4 Casual Sneakers With Sneakers ...   \n",
      "3                Chevit  171 Smart Tan Lace-Ups Casuals for Men Sneaker...   \n",
      "4            Shoes Bank     White Sneaker For Men's/Boy's Sneakers For Men   \n",
      "..                  ...                                                ...   \n",
      "95  World Wear Footwear  Combo Pack of 2 Latest Collection Stylish casu...   \n",
      "96             ASTEROID  Feather Print New Spring Season White Shoes Me...   \n",
      "97              Edoeviv                                   Sneakers For Men   \n",
      "98                 Aura                      Casual Shoes Sneakers For Men   \n",
      "99                 Puma  Puma Smash v2 L Perf Puma White-Puma Whi Sneak...   \n",
      "\n",
      "    Price Discount  \n",
      "0     369       26  \n",
      "1     499       50  \n",
      "2     579       70  \n",
      "3     249       50  \n",
      "4     398       60  \n",
      "..    ...      ...  \n",
      "95    499       50  \n",
      "96    500       49  \n",
      "97    398       43  \n",
      "98    367       26  \n",
      "99  1,599       60  \n",
      "\n",
      "[100 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "driver=webdriver.Chrome(executable_path='D:\\chromedriver.exe')\n",
    "# Function Definition\n",
    "def data_sci(url):\n",
    "    brand, pro_d, pri, dis = [], [], [], []\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    driver.find_element_by_xpath(\"/html/body/div/div/div[1]/div[1]/div[2]/div[2]/form/div/div/input\").send_keys('sneakers')\n",
    "    time.sleep(3)\n",
    "    \n",
    "    driver.find_element_by_xpath(\"/html/body/div[2]/div/div/button\").click()\n",
    "    time.sleep(2)\n",
    "    \n",
    "    driver.find_element_by_xpath(\"/html/body/div/div/div[1]/div[1]/div[2]/div[2]/form/div/button\").click()\n",
    "    time.sleep(3)\n",
    "    \n",
    "    for g in range(3):\n",
    "        g = g + 1\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        jobs = soup.find_all('div', attrs ={'class':'_2WkVRV'})\n",
    "        for j in jobs:\n",
    "            brand.append(j.text)\n",
    "        jobs2 = soup.find_all('a', attrs ={'class':'IRpwTa'})\n",
    "        for k in jobs2:\n",
    "            pro_d.append(k.text)\n",
    "            \n",
    "        jobs1 = soup.find_all('a', attrs ={'class':'_3bPFwb'})\n",
    "        for i in jobs1:          \n",
    "            pri1 = str(i.text).split(\"₹\")[1] \n",
    "            pri.append(pri1)\n",
    "            \n",
    "        for i in jobs1:\n",
    "            try:\n",
    "                dis1 = str(((i.text).split(\"₹\")[2]).split(\"%\")[0])[-2:]\n",
    "                dis.append(dis1)\n",
    "            except:\n",
    "                dis.append('-')\n",
    "\n",
    "        try:\n",
    "            driver.find_element_by_xpath(\"/html/body/div[1]/div/div[3]/div[2]/div[1]/div[2]/div[12]/div/div/nav/a[\"+str(g+10)+\"]\").click()\n",
    "            time.sleep(5)\n",
    "        except NoSuchElementException:\n",
    "            pass\n",
    "             \n",
    "#     print(len(brand))\n",
    "#     print(len(pro_d))\n",
    "#     print(len(pri))\n",
    "#     print(len(dis))     \n",
    "#     print(brand)\n",
    "#     print(pro_d)\n",
    "#     print(pri)\n",
    "#     print(dis)  \n",
    "    \n",
    "    df=pd.DataFrame({'Brand':brand[:100],\n",
    "                     'Product description':pro_d[:100],\n",
    "                    'Price':pri[:100],\n",
    "                     'Discount':dis[:100]})\n",
    "    print(df)\n",
    "    df.to_csv('flipkart_sneakers_100.csv', index = False)\n",
    "    \n",
    "# Calling Function\n",
    "try:\n",
    "    data_sci(\"https://www.flipkart.com/\")\n",
    "    driver.quit()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q9: Go to the link - https://www.myntra.com/shoes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n",
      "100\n",
      "               Brand                          Description  Price\n",
      "0               Nike        Men FREE RN 5.0 Running Shoes   7995\n",
      "1   ADIDAS Originals                Men POD-S3.1 Sneakers  11999\n",
      "2               ALDO         Men Textured Leather Loafers  12999\n",
      "3          Cole Haan  Men Traveller Leather Penny Loafers   9999\n",
      "4               Geox            Men Leather Formal Derbys  11490\n",
      "..               ...                                  ...    ...\n",
      "95              FILA         Heritage Stackhouse Sneakers   8999\n",
      "96      UNDER ARMOUR       Women HOVR Rise Training Shoes   8499\n",
      "97              Geox             Women Leather Flat Boots  12999\n",
      "98  ADIDAS Originals                Men LXCON 94 Sneakers  11999\n",
      "99         Cole Haan          Women Woven Design Sneakers  11249\n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "driver=webdriver.Chrome(executable_path='D:\\chromedriver.exe')\n",
    "# Function Definition\n",
    "def data_sci(url):\n",
    "    brand, dec, pri = [], [], []\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    p_cb = driver.find_element_by_xpath('/html/body/div[2]/div/div[1]/main/div[3]/div[1]/section/div/div[5]/ul/li[2]/label/div')\n",
    "    action = ActionChains(driver) \n",
    "    action.click(on_element=p_cb)\n",
    "    action.perform() \n",
    "    time.sleep(3)\n",
    "    \n",
    "    c_cb = driver.find_element_by_xpath('/html/body/div[2]/div/div[1]/main/div[3]/div[1]/section/div/div[6]/ul/li[1]/label/div')\n",
    "    action = ActionChains(driver) \n",
    "    action.click(on_element=c_cb) \n",
    "    action.perform() \n",
    "    time.sleep(3)\n",
    "    \n",
    "    for g in range(2):\n",
    "        g = g + 1\n",
    "    \n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        jobs = soup.find_all('h3', attrs ={'class':'product-brand'})\n",
    "        for i in jobs:\n",
    "            brand.append(i.text)\n",
    "\n",
    "        jobs1 = soup.find_all('h4', attrs ={'class':'product-product'})\n",
    "        for j in jobs1:\n",
    "            dec.append(j.text)\n",
    "\n",
    "        jobs2 = soup.find_all('div', attrs ={'class':'product-price'})\n",
    "        for k in jobs2:\n",
    "            pri.append((k.text).split(\"Rs. \")[1])\n",
    "\n",
    "        driver.find_element_by_xpath(\"/html/body/div[2]/div/div[1]/main/div[3]/div[2]/div/div[2]/section/div[2]/ul/li[3]/a\").click()\n",
    "        time.sleep(5)\n",
    "                          \n",
    "    print(len(brand))\n",
    "    print(len(dec))\n",
    "    print(len(pri))\n",
    "     \n",
    "#     print(brand)\n",
    "#     print(dec)\n",
    "#     print(pri)\n",
    "    \n",
    "    df=pd.DataFrame({'Brand':brand[:100],\n",
    "                     'Description':dec[:100],\n",
    "                    'Price':pri[:100]})\n",
    "    print(df)\n",
    "    df.to_csv('mintra_shoes_filtered_100.csv', index = False)\n",
    "\n",
    "    \n",
    "# Calling Function\n",
    "try:\n",
    "    data_sci(\"https://www.myntra.com/shoes\")\n",
    "    driver.quit()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q10: Go to webpage https://www.amazon.in/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "25\n",
      "25\n",
      "['Dell Inspiron 5501 15.6 Inch FHD Laptop (10th Gen i7-1065G7/8 GB/512 SSD/2 GB Nvidia Graphics/Win 10 + MS Office H&S 2019/Silver) D560213WIN9S', 'Microsoft Laptop 2 1769 13.5-inch Laptop (Intel Core i7/8GB/256GB SSD/Windows 10 Home/Integrated Graphics), Platinum', 'Asus VivoBook S14 Core i7 8th Gen - (8 GB/1 TB HDD/256 GB SSD/Windows 10 Home/2 GB Graphics) S430FN-EB059T Thin and Light Laptop\\xa0(14 inch, Gun Metal, 1.4 kg)', 'HP Pavilion x360 Core i7 8th Gen 14-inch Touchscreen 2-in-1 FHD Thin and Light Laptop (16GB/512GB SSD/Windows 10/MS Office/2GB Graphics/Mineral Silver/1.59 kg), 14- dh0045TX', 'Asus VivoBook Gaming (2020) Core i7 10th Gen - (16 GB/512 GB SSD/Windows 10 Home/4 GB Graphics/NVIDIA Geforce GTX 1650/120 Hz) F571LH-AL150T Gaming Laptop (15.6 inch, Star Black, 2.14 kg)', 'Lenovo IdeaPad Gaming 3i 10th Gen Intel Core i7 15.6-inch Full HD IPS Gaming Laptop (8GB/1TB HDD + 256 GB SSD/Windows 10/NVIDIA GTX 1650 4GB GDDR6 Graphics/Onyx Black/2.2Kg), 81Y400VAIN', 'ASUS ZenBook 13 (2020) Intel Core i7-1165G7 11th Gen 13.3-inch FHD Thin and Light Laptop (16GB RAM/1TB NVMe SSD/Windows 10/MS Office 2019/Intel Iris Xᵉ Graphics/Lilac Mist/1.11 kg), UX325EA-EG701TS', 'Asus ROG Strix G15 Core i7 10th Gen - (8 GB/512 GB SSD/Windows 10 Home/4 GB Graphics/NVIDIA Geforce GTX 1650 Ti/144 Hz) G512LI-HN081T Gaming Laptop (15.6 inch, Black Plastic, 2.30 kg)', 'Asus ROG Strix G15 (2020) Core i7 10th Gen - (16 GB/1 TB SSD/Windows 10 Home/4 GB Graphics/NVIDIA Geforce GTX 1650 Ti/144 Hz) G512LI-HN086T Gaming Laptop (15.6 inch, Black Plastic, 2.30 kg)', 'ASUS ZenBook Pro Duo Intel Core i9-10980HK 10th Gen 15.6\" 4K UHD OLED Touchscreen Laptop (32GB RAM/1TB NVMe SSD/Windows 10/6GB NVIDIA GeForce RTX 2060 Graphics/Celestial Blue/2.5 Kg), UX581LV-H2035T', '(Renewed) Lenovo ThinkPad High Performance 12.5 inch IPS Panel 1.53 kg Laptop (Core I7 5600U/8 GB(Upto 16GB)/1 TB/Win 10 Pro/Webcam/USB 3.0/SIM/BT/M.2/Cardreader/TPM 2.0 VPro/Integrated Graphics)', 'ASUS ZenBook 14 (2020) Intel Core i7-1165G7 11th Gen 14-inch FHD Thin and Light Laptop (16GB RAM/512GB NVMe SSD/Windows 10/MS Office 2019/Intel Iris Xᵉ Graphics/Pine Grey/1.17 kg), UX425EA-BM701TS', 'Lenovo Legion 5Pi 10th Gen Intel Core i7 15.6\" FHD Gaming Laptop (16GB/1TB SSD/Windows 10/MS Office 2019/144 Hz/NVIDIA RTX 2060 6GB GDDR6/with M300 RGB Gaming Mouse/Iron Grey/2.3Kg), 82AW005SIN', 'ASUS TUF Gaming F15, 15.6\" FHD 144Hz, Intel Core i7-10870H 10th Gen, GTX\\xa01650\\xa0Ti GDDR6 4GB Graphics, Gaming Laptop (8GB RAM/1TB HDD + 512GB SSD/Windows 10/Fortress Gray/2.3 Kg), FX566LI-HN132T', 'ASUS TUF Gaming F15 Laptop 15.6\" FHD Intel Core i7 10th Gen, GeForce GTX 1650 4GB GDDR6 Graphics (8GB RAM/512GB NVMe SSD/Windows 10/Fortress Gray/2.30 Kg), FX566LH-BQ036T + Xbox Game Pass for PC', 'HP Pavilion x360 Touchscreen 2-in-1 FHD 14-inch Laptop (10th Gen Core i7-10510U/8GB/512GB SSD/Windows 10 Home/MS Office/Mineral Silver/1.58 kg), 14-dh1180tu', 'MSI Gaming GL65 Leopard , Intel 9th Gen. i7-9750H, 15.6\" FHD Gaming Laptop (8GB/512GB NVMe SSD/Windows 10 Home/Nvidia GTX 1650/ Black/2.3Kg) 9SCXK-076IN', 'HP 14 10th Gen Intel Core i7 Ultra Thin and Light FHD Laptop (i7-1065G7/8GB/512GB SSD/Win 10/MS Office 2019/Natural Silver/1.46 Kg), 14s-DR1010TU', 'Dell Alienware m15(R3) 15.6-inch FHD Gaming Laptop (10th Gen Core i7-10750H/16GB/512GB SSD/Windows 10 Home & MS Office/6GB NVIDIA GTX 1660 Ti Graphics), Lunar Light', 'Lenovo ThinkPad E14 Intel Core i7 10th Gen14-inch Full HD IPS Thin and Light Laptop (16GB RAM/ 1TB HDD+256GB SSD/ Windows 10 Home/ Microsoft Office Home & Student 2019/ Black/ 1.69kg), 20RAS0AM00', \"(Renewed) Dell Inspiron 3567 Laptop Core i3-7th/6GB/1TB/15.6''/WIN 10 HOME\", '(Renewed) HP EliteBook 1030 G2 X 360 Notebook PC 13.3 inch Laptops (Intel® CoreTM i7-7500U Processor | 8GB RAM | 512GB SSD | Win10 PRO | Integrated Graphics ), Silver', 'Acer Nitro 5 Intel Core i7 10750H 15.6\" FHD IPS 144Hz Display Thin and Light Gaming Laptop (8GB Ram/1TB HDD + 256 GB SSD/Windows 10 Home /GTX 1650Ti Graphics/Obisidian Black/2.3 kgs),AN515-55', '(Renewed) Lenovo ThinkPad High Performance 12.5 inch IPS Panel 1.53 kg Laptop (Core I7 5600U/8 GB(Upto 16GB)/500 GB/Win 10 Pro/Webcam/USB 3.0/SIM/BT/M.2/Cardreader/TPM 2.0 VPro/Integrated Graphics)', 'ASUS VivoBook Ultra 15 (2020) Intel Core i7-1165G7 11th Gen 15.6-inch FHD Thin and Light Laptop (8GB/512GB NVMe SSD/Integrated Graphics/Windows 10/MS Office 2019/Dreamy White/1.8 kg), X513EA-EJ733TS']\n",
      "['3.6', '-', '-', '4.1', '-', '-', '-', '-', '-', '2.8', '3.5', '5.0', '4.3', '-', '3.5', '3.5', '4.2', '3.4', '2.9', '-', '-', '2.3', '4.0', '-', '-']\n",
      "['84,990', '1,50,245', '69,990', '85,990', '81,990', '78,990', '1,01,685', '84,990', '1,04,980', '2,69,294', '34,999', '95,840', '-', '94,999', '73,990', '74,990', '69,990', '71,390', '1,99,900', '84,990', '29,990', '49,990', '79,990', '35,999', '69,990']\n",
      "                                               Title Rating     Price\n",
      "0  Dell Inspiron 5501 15.6 Inch FHD Laptop (10th ...    3.6    84,990\n",
      "1  Microsoft Laptop 2 1769 13.5-inch Laptop (Inte...      -  1,50,245\n",
      "2  Asus VivoBook S14 Core i7 8th Gen - (8 GB/1 TB...      -    69,990\n",
      "3  HP Pavilion x360 Core i7 8th Gen 14-inch Touch...    4.1    85,990\n",
      "4  Asus VivoBook Gaming (2020) Core i7 10th Gen -...      -    81,990\n",
      "5  Lenovo IdeaPad Gaming 3i 10th Gen Intel Core i...      -    78,990\n",
      "6  ASUS ZenBook 13 (2020) Intel Core i7-1165G7 11...      -  1,01,685\n",
      "7  Asus ROG Strix G15 Core i7 10th Gen - (8 GB/51...      -    84,990\n",
      "8  Asus ROG Strix G15 (2020) Core i7 10th Gen - (...      -  1,04,980\n",
      "9  ASUS ZenBook Pro Duo Intel Core i9-10980HK 10t...    2.8  2,69,294\n"
     ]
    }
   ],
   "source": [
    "driver=webdriver.Chrome(executable_path='D:\\chromedriver.exe')\n",
    "# Function Definition\n",
    "def data_sci(url):\n",
    "    title, rating, price = [], [], []\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    jobs = soup.find_all('div', attrs ={'data-component-type':'s-search-result'})\n",
    "    for g in jobs:\n",
    "        titl = (g.text).split(\"\\n\\n\\n\\n\\n\")[5]\n",
    "        if titl == '':\n",
    "            titl = (g.text).split(\"\\n\\n\\n\\n\\n\")[8] \n",
    "            title.append(titl.lstrip(\"\\n\\n\\n\"))\n",
    "        else:\n",
    "            title.append(titl.lstrip(\"\\n\"))\n",
    "                         \n",
    "        try:\n",
    "            rate = str((g.text).split(\" out of 5 stars\")[0][-3:]).replace(\"\\n\\n\\n\",\"-\") \n",
    "            rating.append(rate)\n",
    "        except:\n",
    "            rating.append(\"-\")\n",
    "        try:\n",
    "            pr = (g.text).split(\"₹\")[1] \n",
    "            price.append(pr)\n",
    "        except:\n",
    "            price.append(\"-\")\n",
    "    \n",
    "    print(len(title))\n",
    "    print(len(rating))\n",
    "    print(len(price))\n",
    "     \n",
    "    print(title)\n",
    "    print(rating)\n",
    "    print(price)\n",
    "    \n",
    "    df=pd.DataFrame({'Title':title[:10],\n",
    "                     'Rating':rating[:10],\n",
    "                    'Price':price[:10]})\n",
    "    print(df)\n",
    "    df.to_csv('amazon_laptop_filtered_10.csv', index = False)\n",
    "# Calling Function\n",
    "data_sci(\"https://www.amazon.in/s?k=Laptop&i=computers&rh=n%3A1375424031%2Cp_n_feature_thirteen_browse-bin%3A12598163031%7C16757432031&dc&qid=1608111937&rnid=12598141031&ref=sr_nr_p_n_feature_thirteen_browse-bin_10\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
